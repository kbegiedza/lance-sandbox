{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -q --upgrade pip\n",
    "!python3 -m pip install -q lancedb\n",
    "!python3 -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "uri = r'../data/test.lancedb'\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import cv2\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    x: int\n",
    "    y: int\n",
    "    width: int\n",
    "    height: int\n",
    "\n",
    "@dataclass\n",
    "class Annotation:\n",
    "    className: str\n",
    "    bbox: BoundingBox\n",
    "\n",
    "@dataclass\n",
    "class AnnotatedImage:\n",
    "    image: cv2.Mat\n",
    "    annotations: list[Annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "from xml.etree import ElementTree\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_annotated_images_generator(dataset_path: Path) -> Generator[AnnotatedImage, None, None]:\n",
    "    for file in dataset_path.iterdir():\n",
    "        if not file.name.endswith('.xml'):\n",
    "            continue\n",
    "\n",
    "        parsed_annotation = ElementTree.parse(file)\n",
    "        image_internal_path = parsed_annotation.find('path')\n",
    "\n",
    "        image_path = dataset_path.joinpath(image_internal_path.text)\n",
    "        image = cv2.imread(str(image_path))\n",
    "\n",
    "        objects = parsed_annotation.findall('object')\n",
    "\n",
    "        annotations: list[Annotation] = []\n",
    "\n",
    "        for o in objects:\n",
    "            object_name = o.find('name').text\n",
    "            object_bbox = o.find('bndbox')\n",
    "\n",
    "            bbox_xmin = round(float(object_bbox.find('xmin').text))\n",
    "            bbox_ymin = round(float(object_bbox.find('ymin').text))\n",
    "            bbox_xmax = round(float(object_bbox.find('xmax').text))\n",
    "            bbox_ymax = round(float(object_bbox.find('ymax').text))\n",
    "\n",
    "            bounding_box = BoundingBox(bbox_xmin, bbox_ymin, bbox_xmax - bbox_xmin, bbox_ymax - bbox_ymin)\n",
    "            annotation = Annotation(object_name, bounding_box)\n",
    "\n",
    "            annotations.append(annotation)\n",
    "\n",
    "        yield AnnotatedImage(image, annotations)\n",
    "\n",
    "def encode_image(path_to_image: Path):\n",
    "    image = cv2.imread(path_to_image)\n",
    "    encoding = path_to_image.suffix\n",
    "\n",
    "    return cv2.imencode(encoding, image)[1].tobytes()\n",
    "\n",
    "\n",
    "def decode_image(encoded_image):\n",
    "    nparr = np.frombuffer(encoded_image, np.byte)\n",
    "    return cv2.imdecode(nparr, cv2.IMREAD_ANYCOLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(r'../data/sample-dataset/')\n",
    "\n",
    "test_dataset_path = dataset_path.joinpath('test')\n",
    "\n",
    "for item in get_annotated_images_generator(test_dataset_path):\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
